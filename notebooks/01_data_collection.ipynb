{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0b1519",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c5105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yfinance as yf\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8d457",
   "metadata": {},
   "source": [
    "\n",
    "# IPO Data Collection: Curated Tech IPO List\n",
    "\n",
    "Manual compilation from public sources (2018-2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3a3225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CURATED TECH IPO DATASET\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Summary:\n",
      "   Total IPOs: 87\n",
      "   Date range: 2018-03-16 to 2024-06-14\n",
      "   Sectors: 58\n",
      "\n",
      "üìà IPOs by Year:\n",
      "IPO_Year\n",
      "2018    14\n",
      "2019    21\n",
      "2020    19\n",
      "2021    21\n",
      "2022     2\n",
      "2023     5\n",
      "2024     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üè¢ Sample IPOs:\n",
      "             Company Ticker   IPO_Date Lockup_Expiration              Sector\n",
      "0             Reddit   RDDT 2024-03-21        2024-09-17        Social Media\n",
      "1        Astera Labs   ALAB 2024-03-20        2024-09-16      Semiconductors\n",
      "2             Rubrik   RBRK 2024-04-27        2024-10-24      Cloud Security\n",
      "3             Ibotta   IBTA 2024-04-18        2024-10-15          E-commerce\n",
      "4          Tempus AI    TEM 2024-06-14        2024-12-11     Healthcare Tech\n",
      "5       Arm Holdings    ARM 2023-09-14        2024-03-12      Semiconductors\n",
      "6          Instacart   CART 2023-09-19        2024-03-17          E-commerce\n",
      "7            Klaviyo   KVYO 2023-09-20        2024-03-18      Marketing Tech\n",
      "8        Birkenstock   BIRK 2023-10-11        2024-04-08            Consumer\n",
      "9         Nextracker    NXT 2023-02-09        2023-08-08        Clean Energy\n",
      "10               TPG    TPG 2022-01-13        2022-07-12  Financial Services\n",
      "11  Credo Technology   CRDO 2022-02-03        2022-08-02      Semiconductors\n",
      "12            Rivian   RIVN 2021-11-10        2022-05-09          Automotive\n",
      "13          Coinbase   COIN 2021-04-14        2021-10-11             Fintech\n",
      "14            Roblox   RBLX 2021-03-10        2021-09-06              Gaming\n",
      "\n",
      "üíæ Saved to: ../data/processed/tech_ipos_curated.csv\n",
      "\n",
      "‚úÖ Ready for stock price download!\n"
     ]
    }
   ],
   "source": [
    "# Curated list of major tech IPOs 2018-2024\n",
    "tech_ipos_data = {\n",
    "    'Company': [\n",
    "        # 2024\n",
    "        'Reddit', 'Astera Labs', 'Rubrik', 'Ibotta', 'Tempus AI',\n",
    "        \n",
    "        # 2023  \n",
    "        'Arm Holdings', 'Instacart', 'Klaviyo', 'Birkenstock', 'Nextracker',\n",
    "        \n",
    "        # 2022\n",
    "        'TPG', 'Credo Technology',\n",
    "        \n",
    "        # 2021\n",
    "        'Rivian', 'Coinbase', 'Roblox', 'UiPath', 'Bumble', 'AppLovin',\n",
    "        'Robinhood', 'Toast', 'Warby Parker', 'Freshworks', 'GitLab',\n",
    "        'Affirm', 'SoFi', 'Marqeta', 'ZipRecruiter', 'monday.com',\n",
    "        'Sprinklr', 'SentinelOne', 'Kaltura', 'PowerSchool', 'Amplitude',\n",
    "        \n",
    "        # 2020\n",
    "        'Snowflake', 'Airbnb', 'DoorDash', 'Palantir', 'Asana',\n",
    "        'Unity Software', 'JFrog', 'BigCommerce', 'Sumo Logic', 'Lemonade',\n",
    "        'nCino', 'Vroom', 'Shift4 Payments', 'Jamf', 'C3.ai',\n",
    "        'ZoomInfo', 'Datto', 'Corsair Gaming', 'ContextLogic (Wish)',\n",
    "        \n",
    "        # 2019\n",
    "        'Uber', 'Lyft', 'Slack', 'Pinterest', 'Zoom', 'CrowdStrike',\n",
    "        'Datadog', 'PagerDuty', 'Elastic', 'Fastly', 'Tradeweb',\n",
    "        'Cloudflare', 'Dynatrace', 'Peloton', 'Chewy', 'Change Healthcare',\n",
    "        'Tufin', 'Revolve', 'Jumia', 'Beyond Meat', 'Fiverr',\n",
    "        \n",
    "        # 2018\n",
    "        'Spotify', 'Dropbox', 'DocuSign', 'SmartSheet', 'Zuora',\n",
    "        'Carbon Black', 'Pivotal Software', 'Zscaler', 'iQIYI',\n",
    "        'Tencent Music', 'Nio', 'Farfetch', 'MongoDB', 'SurveyMonkey'\n",
    "    ],\n",
    "    \n",
    "    'Ticker': [\n",
    "        # 2024\n",
    "        'RDDT', 'ALAB', 'RBRK', 'IBTA', 'TEM',\n",
    "        \n",
    "        # 2023\n",
    "        'ARM', 'CART', 'KVYO', 'BIRK', 'NXT',\n",
    "        \n",
    "        # 2022\n",
    "        'TPG', 'CRDO',\n",
    "        \n",
    "        # 2021\n",
    "        'RIVN', 'COIN', 'RBLX', 'PATH', 'BMBL', 'APP',\n",
    "        'HOOD', 'TOST', 'WRBY', 'FRSH', 'GTLB',\n",
    "        'AFRM', 'SOFI', 'MQ', 'ZIP', 'MNDY',\n",
    "        'CXM', 'S', 'KLTR', 'PWSC', 'AMPL',\n",
    "        \n",
    "        # 2020\n",
    "        'SNOW', 'ABNB', 'DASH', 'PLTR', 'ASAN',\n",
    "        'U', 'FROG', 'BIGC', 'SUMO', 'LMND',\n",
    "        'NCNO', 'VRM', 'FOUR', 'JAMF', 'AI',\n",
    "        'ZI', 'DATTO', 'CRSR', 'WISH',\n",
    "        \n",
    "        # 2019\n",
    "        'UBER', 'LYFT', 'WORK', 'PINS', 'ZM', 'CRWD',\n",
    "        'DDOG', 'PD', 'ESTC', 'FSLY', 'TW',\n",
    "        'NET', 'DT', 'PTON', 'CHWY', 'CHNG',\n",
    "        'TUFN', 'RVLV', 'JMIA', 'BYND', 'FVRR',\n",
    "        \n",
    "        # 2018\n",
    "        'SPOT', 'DBX', 'DOCU', 'SMAR', 'ZUO',\n",
    "        'CBLK', 'PVTL', 'ZS', 'IQ',\n",
    "        'TME', 'NIO', 'FTCH', 'MDB', 'SVMK'\n",
    "    ],\n",
    "    \n",
    "    'IPO_Date': [\n",
    "        # 2024\n",
    "        '2024-03-21', '2024-03-20', '2024-04-27', '2024-04-18', '2024-06-14',\n",
    "        \n",
    "        # 2023\n",
    "        '2023-09-14', '2023-09-19', '2023-09-20', '2023-10-11', '2023-02-09',\n",
    "        \n",
    "        # 2022\n",
    "        '2022-01-13', '2022-02-03',\n",
    "        \n",
    "        # 2021\n",
    "        '2021-11-10', '2021-04-14', '2021-03-10', '2021-04-21', '2021-02-11', '2021-04-12',\n",
    "        '2021-07-29', '2021-09-22', '2021-09-29', '2021-09-22', '2021-10-14',\n",
    "        '2021-01-13', '2021-01-07', '2021-06-09', '2021-05-26', '2021-06-10',\n",
    "        '2021-06-23', '2021-06-30', '2021-07-21', '2021-08-11', '2021-09-28',\n",
    "        \n",
    "        # 2020\n",
    "        '2020-09-16', '2020-12-10', '2020-12-09', '2020-09-30', '2020-09-30',\n",
    "        '2020-09-18', '2020-09-16', '2020-08-05', '2020-09-17', '2020-07-02',\n",
    "        '2020-07-14', '2020-06-09', '2020-06-09', '2020-07-22', '2020-12-09',\n",
    "        '2020-06-04', '2020-10-21', '2020-09-23', '2020-12-16',\n",
    "        \n",
    "        # 2019\n",
    "        '2019-05-10', '2019-03-29', '2019-06-20', '2019-04-18', '2019-04-18', '2019-06-12',\n",
    "        '2019-09-19', '2019-04-11', '2019-10-03', '2019-05-17', '2019-04-04',\n",
    "        '2019-09-13', '2019-08-01', '2019-09-26', '2019-06-14', '2019-06-27',\n",
    "        '2019-04-11', '2019-06-07', '2019-04-12', '2019-05-02', '2019-06-13',\n",
    "        \n",
    "        # 2018\n",
    "        '2018-04-03', '2018-03-23', '2018-04-27', '2018-04-27', '2018-04-12',\n",
    "        '2018-05-03', '2018-04-20', '2018-03-16', '2018-03-29',\n",
    "        '2018-12-12', '2018-09-12', '2018-09-21', '2018-10-11', '2018-09-26'\n",
    "    ],\n",
    "    \n",
    "    'Sector': [\n",
    "        # 2024\n",
    "        'Social Media', 'Semiconductors', 'Cloud Security', 'E-commerce', 'Healthcare Tech',\n",
    "        \n",
    "        # 2023\n",
    "        'Semiconductors', 'E-commerce', 'Marketing Tech', 'Consumer', 'Clean Energy',\n",
    "        \n",
    "        # 2022\n",
    "        'Financial Services', 'Semiconductors',\n",
    "        \n",
    "        # 2021\n",
    "        'Automotive', 'Fintech', 'Gaming', 'Enterprise Software', 'Social', 'Mobile Gaming',\n",
    "        'Fintech', 'Payments', 'E-commerce', 'Enterprise Software', 'DevOps',\n",
    "        'Fintech', 'Fintech', 'Payments', 'HR Tech', 'Productivity',\n",
    "        'Marketing', 'Cybersecurity', 'Video', 'Education Tech', 'Analytics',\n",
    "        \n",
    "        # 2020\n",
    "        'Cloud Data', 'Travel', 'Delivery', 'Big Data', 'Productivity',\n",
    "        'Gaming', 'DevOps', 'E-commerce', 'Cloud', 'Insurtech',\n",
    "        'Fintech', 'E-commerce', 'Payments', 'Device Management', 'AI',\n",
    "        'Sales Tech', 'IT Management', 'Gaming', 'E-commerce',\n",
    "        \n",
    "        # 2019\n",
    "        'Rideshare', 'Rideshare', 'Enterprise Collaboration', 'Social Media', 'Video Conferencing', 'Cybersecurity',\n",
    "        'Cloud Monitoring', 'DevOps', 'Search/Analytics', 'Edge Computing', 'Financial Tech',\n",
    "        'Edge Computing', 'Application Performance', 'Fitness', 'E-commerce', 'Healthcare IT',\n",
    "        'Cybersecurity', 'Fashion E-commerce', 'E-commerce', 'Food Tech', 'Freelance Platform',\n",
    "        \n",
    "        # 2018\n",
    "        'Music Streaming', 'Cloud Storage', 'E-signature', 'Collaboration', 'Subscription Billing',\n",
    "        'Cybersecurity', 'Cloud Platform', 'Cloud Security', 'Video Streaming',\n",
    "        'Music Streaming', 'Electric Vehicles', 'Luxury E-commerce', 'Database', 'Survey Software'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_ipos = pd.DataFrame(tech_ipos_data)\n",
    "\n",
    "# Convert date\n",
    "df_ipos['IPO_Date'] = pd.to_datetime(df_ipos['IPO_Date'])\n",
    "\n",
    "# Calculate lockup expiration (180 days standard)\n",
    "df_ipos['Lockup_Expiration'] = df_ipos['IPO_Date'] + pd.Timedelta(days=180)\n",
    "\n",
    "# Add year for easier filtering\n",
    "df_ipos['IPO_Year'] = df_ipos['IPO_Date'].dt.year\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CURATED TECH IPO DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"   Total IPOs: {len(df_ipos)}\")\n",
    "print(f\"   Date range: {df_ipos['IPO_Date'].min().date()} to {df_ipos['IPO_Date'].max().date()}\")\n",
    "print(f\"   Sectors: {df_ipos['Sector'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìà IPOs by Year:\")\n",
    "print(df_ipos['IPO_Year'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nüè¢ Sample IPOs:\")\n",
    "print(df_ipos[['Company', 'Ticker', 'IPO_Date', 'Lockup_Expiration', 'Sector']].head(15))\n",
    "\n",
    "# Save\n",
    "output_path = f'{output_dir}/tech_ipos_curated.csv'\n",
    "df_ipos.to_csv(output_path, index=False)\n",
    "print(f\"\\nüíæ Saved to: {output_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready for stock price download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8632107",
   "metadata": {},
   "source": [
    "# Stock price download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4cc581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading stock prices for 87 IPOs...\n",
      "   RDDT   (Reddit                        )... ‚úÖ 250 days\n",
      "   ALAB   (Astera Labs                   )... ‚úÖ 250 days\n",
      "   RBRK   (Rubrik                        )... ‚úÖ 249 days\n",
      "   IBTA   (Ibotta                        )... ‚úÖ 250 days\n",
      "   TEM    (Tempus AI                     )... ‚úÖ 250 days\n",
      "   ARM    (Arm Holdings                  )... ‚úÖ 251 days\n",
      "   CART   (Instacart                     )... ‚úÖ 251 days\n",
      "   KVYO   (Klaviyo                       )... ‚úÖ 251 days\n",
      "   BIRK   (Birkenstock                   )... ‚úÖ 251 days\n",
      "   NXT    (Nextracker                    )... ‚úÖ 251 days\n",
      "\n",
      "   Processed 10/87...\n",
      "\n",
      "   TPG    (TPG                           )... ‚úÖ 251 days\n",
      "   CRDO   (Credo Technology              )... ‚úÖ 251 days\n",
      "   RIVN   (Rivian                        )... ‚úÖ 252 days\n",
      "   COIN   (Coinbase                      )... ‚úÖ 254 days\n",
      "   RBLX   (Roblox                        )... ‚úÖ 253 days\n",
      "   PATH   (UiPath                        )... ‚úÖ 253 days\n",
      "   BMBL   (Bumble                        )... ‚úÖ 253 days\n",
      "   APP    (AppLovin                      )... ‚úÖ 251 days\n",
      "   HOOD   (Robinhood                     )... ‚úÖ 252 days\n",
      "   TOST   (Toast                         )... ‚úÖ 252 days\n",
      "\n",
      "   Processed 20/87...\n",
      "\n",
      "   WRBY   (Warby Parker                  )... ‚úÖ 252 days\n",
      "   FRSH   (Freshworks                    )... ‚úÖ 252 days\n",
      "   GTLB   (GitLab                        )... ‚úÖ 252 days\n",
      "   AFRM   (Affirm                        )... ‚úÖ 253 days\n",
      "   SOFI   (SoFi                          )... ‚úÖ 253 days\n",
      "   MQ     (Marqeta                       )... ‚úÖ 253 days\n",
      "   ZIP    (ZipRecruiter                  )... ‚úÖ 253 days\n",
      "   MNDY   (monday.com                    )... ‚úÖ 253 days\n",
      "   CXM    (Sprinklr                      )... ‚úÖ 252 days\n",
      "   S      (SentinelOne                   )... ‚úÖ 252 days\n",
      "\n",
      "   Processed 30/87...\n",
      "\n",
      "   KLTR   (Kaltura                       )... ‚úÖ 252 days\n",
      "   PWSC   (PowerSchool                   )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['PWSC']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   AMPL   (Amplitude                     )... ‚úÖ 252 days\n",
      "   SNOW   (Snowflake                     )... ‚úÖ 252 days\n",
      "   ABNB   (Airbnb                        )... ‚úÖ 252 days\n",
      "   DASH   (DoorDash                      )... ‚úÖ 252 days\n",
      "   PLTR   (Palantir                      )... ‚úÖ 252 days\n",
      "   ASAN   (Asana                         )... ‚úÖ 252 days\n",
      "   U      (Unity Software                )... ‚úÖ 252 days\n",
      "   FROG   (JFrog                         )... ‚úÖ 252 days\n",
      "\n",
      "   Processed 40/87...\n",
      "\n",
      "   BIGC   (BigCommerce                   )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: BIGC\"}}}\n",
      "\n",
      "1 Failed download:\n",
      "['BIGC']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   SUMO   (Sumo Logic                    )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SUMO']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   LMND   (Lemonade                      )... ‚úÖ 252 days\n",
      "   NCNO   (nCino                         )... ‚úÖ 252 days\n",
      "   VRM    (Vroom                         )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['VRM']: YFPricesMissingError('possibly delisted; no price data found  (1d 2020-06-09 00:00:00 -> 2021-06-09 00:00:00) (Yahoo error = \"Data doesn\\'t exist for startDate = 1591675200, endDate = 1623211200\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   FOUR   (Shift4 Payments               )... ‚úÖ 252 days\n",
      "   JAMF   (Jamf                          )... ‚úÖ 252 days\n",
      "   AI     (C3.ai                         )... ‚úÖ 252 days\n",
      "   ZI     (ZoomInfo                      )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['ZI']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   DATTO  (Datto                         )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['DATTO']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "\n",
      "   Processed 50/87...\n",
      "\n",
      "   CRSR   (Corsair Gaming                )... ‚úÖ 252 days\n",
      "   WISH   (ContextLogic (Wish)           )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['WISH']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   UBER   (Uber                          )... ‚úÖ 252 days\n",
      "   LYFT   (Lyft                          )... ‚úÖ 252 days\n",
      "   WORK   (Slack                         )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['WORK']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   PINS   (Pinterest                     )... ‚úÖ 251 days\n",
      "   ZM     (Zoom                          )... ‚úÖ 251 days\n",
      "   CRWD   (CrowdStrike                   )... ‚úÖ 252 days\n",
      "   DDOG   (Datadog                       )... ‚úÖ 252 days\n",
      "   PD     (PagerDuty                     )... ‚úÖ 252 days\n",
      "\n",
      "   Processed 60/87...\n",
      "\n",
      "   ESTC   (Elastic                       )... ‚úÖ 252 days\n",
      "   FSLY   (Fastly                        )... ‚úÖ 252 days\n",
      "   TW     (Tradeweb                      )... ‚úÖ 252 days\n",
      "   NET    (Cloudflare                    )... ‚úÖ 252 days\n",
      "   DT     (Dynatrace                     )... ‚úÖ 252 days\n",
      "   PTON   (Peloton                       )... ‚úÖ 252 days\n",
      "   CHWY   (Chewy                         )... ‚úÖ 252 days\n",
      "   CHNG   (Change Healthcare             )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['CHNG']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   TUFN   (Tufin                         )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['TUFN']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   RVLV   (Revolve                       )... ‚úÖ 252 days\n",
      "\n",
      "   Processed 70/87...\n",
      "\n",
      "   JMIA   (Jumia                         )... ‚úÖ 251 days\n",
      "   BYND   (Beyond Meat                   )... ‚úÖ 252 days\n",
      "   FVRR   (Fiverr                        )... ‚úÖ 252 days\n",
      "   SPOT   (Spotify                       )... ‚úÖ 252 days\n",
      "   DBX    (Dropbox                       )... ‚úÖ 251 days\n",
      "   DOCU   (DocuSign                      )... ‚úÖ 251 days\n",
      "   SMAR   (SmartSheet                    )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SMAR']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   ZUO    (Zuora                         )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['ZUO']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   CBLK   (Carbon Black                  )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['CBLK']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   PVTL   (Pivotal Software              )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['PVTL']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "\n",
      "   Processed 80/87...\n",
      "\n",
      "   ZS     (Zscaler                       )... ‚úÖ 251 days\n",
      "   IQ     (iQIYI                         )... ‚úÖ 251 days\n",
      "   TME    (Tencent Music                 )... ‚úÖ 252 days\n",
      "   NIO    (Nio                           )... ‚úÖ 251 days\n",
      "   FTCH   (Farfetch                      )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['FTCH']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "   MDB    (MongoDB                       )... ‚úÖ 251 days\n",
      "   SVMK   (SurveyMonkey                  )... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SVMK']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data\n",
      "\n",
      "‚úÖ Stock prices downloaded:\n",
      "   Companies: 71\n",
      "   Total observations: 17,874\n",
      "   Columns: ['Date', 'Ticker', 'Company', 'IPO_Date', 'Days_Since_IPO', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "\n",
      "üìä Data types:\n",
      "Price\n",
      "Date              datetime64[ns]\n",
      "Ticker                    object\n",
      "Company                   object\n",
      "IPO_Date          datetime64[ns]\n",
      "Days_Since_IPO             int64\n",
      "Open                     float64\n",
      "High                     float64\n",
      "Low                      float64\n",
      "Close                    float64\n",
      "Volume                     int64\n",
      "dtype: object\n",
      "\n",
      "üîç Sample (first 5 rows):\n",
      "Price       Date Ticker Company   IPO_Date  Days_Since_IPO       Open  \\\n",
      "0     2024-03-21   RDDT  Reddit 2024-03-21               0  47.000000   \n",
      "1     2024-03-22   RDDT  Reddit 2024-03-21               1  48.880001   \n",
      "2     2024-03-25   RDDT  Reddit 2024-03-21               4  47.090000   \n",
      "3     2024-03-26   RDDT  Reddit 2024-03-21               5  67.709999   \n",
      "4     2024-03-27   RDDT  Reddit 2024-03-21               6  63.759998   \n",
      "\n",
      "Price       High        Low      Close    Volume  \n",
      "0      57.799999  45.049999  50.439999  48705500  \n",
      "1      51.000000  45.340000  46.000000  15983100  \n",
      "2      61.939999  46.080002  59.799999  24398800  \n",
      "3      74.900002  63.209999  65.110001  35331000  \n",
      "4      63.759998  55.619999  57.750000  18758300  \n",
      "\n",
      "üíæ Saved to: ../data/processed/stock_prices_ipo.csv\n"
     ]
    }
   ],
   "source": [
    "# Load IPO list\n",
    "df_ipos = pd.read_csv('../data/processed/tech_ipos_curated.csv', parse_dates=['IPO_Date', 'Lockup_Expiration'])\n",
    "\n",
    "print(f\"üì• Downloading stock prices for {len(df_ipos)} IPOs...\")\n",
    "\n",
    "# Store each company's data separately, then concat\n",
    "stock_data_list = []\n",
    "\n",
    "for idx, row in df_ipos.iterrows():\n",
    "    ticker = row['Ticker']\n",
    "    ipo_date = row['IPO_Date']\n",
    "    company = row['Company']\n",
    "    \n",
    "    # Download from IPO date to +365 days\n",
    "    start_date = ipo_date\n",
    "    end_date = ipo_date + timedelta(days=365)\n",
    "    \n",
    "    try:\n",
    "        print(f\"   {ticker:6s} ({company:30s})... \", end='')\n",
    "        \n",
    "        # Download for THIS TICKER ONLY\n",
    "        stock = yf.download(\n",
    "            ticker,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            progress=False,\n",
    "            auto_adjust=False  # Keep raw OHLCV\n",
    "        )\n",
    "        \n",
    "        if len(stock) > 0:\n",
    "            # Reset index to make Date a column\n",
    "            stock = stock.reset_index()\n",
    "            \n",
    "            # CRITICAL: Flatten multi-index columns if present\n",
    "            if isinstance(stock.columns, pd.MultiIndex):\n",
    "                stock.columns = stock.columns.get_level_values(0)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            stock['Ticker'] = ticker\n",
    "            stock['Company'] = company\n",
    "            stock['IPO_Date'] = ipo_date\n",
    "            stock['Days_Since_IPO'] = (stock['Date'] - ipo_date).dt.days\n",
    "            \n",
    "            # Reorder columns\n",
    "            cols = ['Date', 'Ticker', 'Company', 'IPO_Date', 'Days_Since_IPO', \n",
    "                    'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            stock = stock[cols]\n",
    "            \n",
    "            stock_data_list.append(stock)\n",
    "            print(f\"‚úÖ {len(stock)} days\")\n",
    "        else:\n",
    "            print(f\"‚ùå No data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)[:50]}\")\n",
    "    \n",
    "    # Rate limiting\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"\\n   Processed {idx + 1}/{len(df_ipos)}...\\n\")\n",
    "        time.sleep(2)\n",
    "\n",
    "# Combine all into single DataFrame (LONG FORMAT)\n",
    "if len(stock_data_list) > 0:\n",
    "    df_stock_prices = pd.concat(stock_data_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Stock prices downloaded:\")\n",
    "    print(f\"   Companies: {df_stock_prices['Ticker'].nunique()}\")\n",
    "    print(f\"   Total observations: {len(df_stock_prices):,}\")\n",
    "    print(f\"   Columns: {df_stock_prices.columns.tolist()}\")\n",
    "    \n",
    "    # Verify data types\n",
    "    print(f\"\\nüìä Data types:\")\n",
    "    print(df_stock_prices.dtypes)\n",
    "    \n",
    "    # Sample check\n",
    "    print(f\"\\nüîç Sample (first 5 rows):\")\n",
    "    print(df_stock_prices.head())\n",
    "    \n",
    "    # Save\n",
    "    output_path = '../data/processed/stock_prices_ipo.csv'\n",
    "    df_stock_prices.to_csv(output_path, index=False)\n",
    "    print(f\"\\nüíæ Saved to: {output_path}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No stock data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab2a56",
   "metadata": {},
   "source": [
    "# MARKET ADJUSTMENT\n",
    "Calculate Abnormal Returns<br>\n",
    "Remove COVID/macro effects by adjusting for S&P 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a67f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/3vxnll4s24b0zx1_rpkbk7r40000gp/T/ipykernel_15167/3506242703.py:45: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  spy = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MARKET ADJUSTMENT\n",
      "================================================================================\n",
      "\n",
      "üìä Stock data loaded:\n",
      "   Companies: 71\n",
      "   Observations: 17,874\n",
      "\n",
      "üîß Fixing data types...\n",
      "   Current 'Close' dtype: float64\n",
      "   ‚úÖ Converted 'Close' to numeric\n",
      "   ‚úÖ Converted 'Open' to numeric\n",
      "   ‚úÖ Converted 'High' to numeric\n",
      "   ‚úÖ Converted 'Low' to numeric\n",
      "   ‚úÖ Converted 'Volume' to numeric\n",
      "   New 'Close' dtype: float64\n",
      "\n",
      "üì• Downloading S&P 500 benchmark...\n",
      "   Raw SPY data shape: (1821, 5)\n",
      "   ‚úÖ Flattened multi-index columns\n",
      "   ‚úÖ S&P 500 data: 1821 trading days\n",
      "\n",
      "üîó Merging with stock data...\n",
      "   ‚úÖ Merged successfully\n",
      "\n",
      "üìà Calculating stock returns...\n",
      "   ‚úÖ Abnormal returns calculated\n",
      "\n",
      "üìä Calculating cumulative abnormal returns...\n",
      "   ‚úÖ Cumulative abnormal returns calculated\n",
      "\n",
      "üìä Summary Statistics:\n",
      "   Total observations: 17,874\n",
      "   Non-null abnormal returns: 17,802\n",
      "   Mean daily abnormal return: 0.0254%\n",
      "   Std daily abnormal return: 4.4802%\n",
      "   Median cum. abnormal return at Day 180: -8.03%\n",
      "   Mean cum. abnormal return at Day 180: -5.07%\n",
      "\n",
      "üîç Sample data (first company, first 5 days):\n",
      "Ticker       Date  Days_Since_IPO      Close  Stock_Return  SPY_Return  Abnormal_Return\n",
      "  ABNB 2020-12-10               0 144.710007           NaN   -0.032713              NaN\n",
      "  ABNB 2020-12-11               1 139.250000     -3.773068   -0.117237        -3.655831\n",
      "  ABNB 2020-12-14               4 130.000000     -6.642729   -0.447745        -6.194984\n",
      "  ABNB 2020-12-15               5 124.800003     -3.999998    1.351942        -5.351940\n",
      "  ABNB 2020-12-16               6 137.990005     10.568912    0.156948        10.411964\n",
      "\n",
      "üìä Data quality:\n",
      "   Missing abnormal returns: 72 (0.40%)\n",
      "   (First day of each IPO expected to be null)\n",
      "\n",
      "üíæ Saved: ../data/processed/stock_prices_ipo_adjusted.csv\n",
      "\n",
      "‚úÖ Data ready for DiD analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/3vxnll4s24b0zx1_rpkbk7r40000gp/T/ipykernel_15167/3506242703.py:99: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_stock = df_stock.groupby('Ticker').apply(calculate_cumulative).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MARKET ADJUSTMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load IPO stock data\n",
    "df_stock = pd.read_csv('../data/processed/stock_prices_ipo.csv', parse_dates=['Date', 'IPO_Date'])\n",
    "\n",
    "print(f\"\\nüìä Stock data loaded:\")\n",
    "print(f\"   Companies: {df_stock['Ticker'].nunique()}\")\n",
    "print(f\"   Observations: {len(df_stock):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIX: Convert numeric columns from string to float\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîß Fixing data types...\")\n",
    "\n",
    "# Check current dtypes\n",
    "print(f\"   Current 'Close' dtype: {df_stock['Close'].dtype}\")\n",
    "\n",
    "# Numeric columns that should be float\n",
    "numeric_cols = ['Close', 'Open', 'High', 'Low', 'Volume']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_stock.columns:\n",
    "        # Convert to numeric, coercing errors to NaN\n",
    "        df_stock[col] = pd.to_numeric(df_stock[col], errors='coerce')\n",
    "        print(f\"   ‚úÖ Converted '{col}' to numeric\")\n",
    "\n",
    "# Verify\n",
    "print(f\"   New 'Close' dtype: {df_stock['Close'].dtype}\")\n",
    "\n",
    "# Check for any conversion issues\n",
    "null_close = df_stock['Close'].isna().sum()\n",
    "if null_close > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {null_close} null values in Close after conversion\")\n",
    "\n",
    "# ============================================================================\n",
    "# Continue with market adjustment\n",
    "# ============================================================================\n",
    "\n",
    "# Download S&P 500 benchmark\n",
    "print(f\"\\nüì• Downloading S&P 500 benchmark...\")\n",
    "\n",
    "spy = yf.download(\n",
    "    'SPY',\n",
    "    start=df_stock['Date'].min(),\n",
    "    end=df_stock['Date'].max(),\n",
    "    progress=False\n",
    ")\n",
    "\n",
    "print(f\"   Raw SPY data shape: {spy.shape}\")\n",
    "\n",
    "# FIX: Flatten multi-index columns if present\n",
    "if isinstance(spy.columns, pd.MultiIndex):\n",
    "    spy.columns = spy.columns.get_level_values(0)\n",
    "    print(f\"   ‚úÖ Flattened multi-index columns\")\n",
    "\n",
    "# Calculate daily returns\n",
    "spy['SPY_Return'] = spy['Close'].pct_change() * 100\n",
    "\n",
    "# Reset index\n",
    "spy_returns = spy[['SPY_Return']].reset_index()\n",
    "\n",
    "# Ensure Date column name\n",
    "if 'index' in spy_returns.columns:\n",
    "    spy_returns.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "print(f\"   ‚úÖ S&P 500 data: {len(spy_returns)} trading days\")\n",
    "\n",
    "# Merge with IPO data\n",
    "print(f\"\\nüîó Merging with stock data...\")\n",
    "\n",
    "df_stock = df_stock.merge(spy_returns, on='Date', how='left')\n",
    "\n",
    "print(f\"   ‚úÖ Merged successfully\")\n",
    "\n",
    "# Calculate daily stock returns\n",
    "print(f\"\\nüìà Calculating stock returns...\")\n",
    "df_stock = df_stock.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# NOW this will work because Close is numeric\n",
    "df_stock['Stock_Return'] = df_stock.groupby('Ticker')['Close'].pct_change() * 100\n",
    "\n",
    "# Calculate ABNORMAL return (stock - market)\n",
    "df_stock['Abnormal_Return'] = df_stock['Stock_Return'] - df_stock['SPY_Return']\n",
    "\n",
    "print(f\"   ‚úÖ Abnormal returns calculated\")\n",
    "\n",
    "# Calculate cumulative abnormal return from IPO\n",
    "print(f\"\\nüìä Calculating cumulative abnormal returns...\")\n",
    "\n",
    "def calculate_cumulative(group):\n",
    "    \"\"\"Cumulative abnormal return from IPO date\"\"\"\n",
    "    group = group.sort_values('Days_Since_IPO')\n",
    "    group['Cum_Abnormal_Return'] = group['Abnormal_Return'].fillna(0).cumsum()\n",
    "    return group\n",
    "\n",
    "df_stock = df_stock.groupby('Ticker').apply(calculate_cumulative).reset_index(drop=True)\n",
    "\n",
    "print(f\"   ‚úÖ Cumulative abnormal returns calculated\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Summary Statistics:\")\n",
    "print(f\"   Total observations: {len(df_stock):,}\")\n",
    "print(f\"   Non-null abnormal returns: {df_stock['Abnormal_Return'].notna().sum():,}\")\n",
    "print(f\"   Mean daily abnormal return: {df_stock['Abnormal_Return'].mean():.4f}%\")\n",
    "print(f\"   Std daily abnormal return: {df_stock['Abnormal_Return'].std():.4f}%\")\n",
    "\n",
    "# Check Day 180 specifically\n",
    "day_180_data = df_stock[df_stock['Days_Since_IPO'] == 180]\n",
    "if len(day_180_data) > 0:\n",
    "    median_180 = day_180_data['Cum_Abnormal_Return'].median()\n",
    "    mean_180 = day_180_data['Cum_Abnormal_Return'].mean()\n",
    "    print(f\"   Median cum. abnormal return at Day 180: {median_180:.2f}%\")\n",
    "    print(f\"   Mean cum. abnormal return at Day 180: {mean_180:.2f}%\")\n",
    "\n",
    "# Sample data check\n",
    "print(f\"\\nüîç Sample data (first company, first 5 days):\")\n",
    "first_ticker = df_stock['Ticker'].iloc[0]\n",
    "sample = df_stock[df_stock['Ticker'] == first_ticker].head(5)\n",
    "print(sample[['Ticker', 'Date', 'Days_Since_IPO', 'Close', 'Stock_Return', 'SPY_Return', 'Abnormal_Return']].to_string(index=False))\n",
    "\n",
    "# Check for missing values\n",
    "missing_abn = df_stock['Abnormal_Return'].isna().sum()\n",
    "pct_missing = (missing_abn / len(df_stock)) * 100\n",
    "print(f\"\\nüìä Data quality:\")\n",
    "print(f\"   Missing abnormal returns: {missing_abn} ({pct_missing:.2f}%)\")\n",
    "print(f\"   (First day of each IPO expected to be null)\")\n",
    "\n",
    "# Save market-adjusted data\n",
    "output_path = '../data/processed/stock_prices_ipo_adjusted.csv'\n",
    "df_stock.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6c30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipoenv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
